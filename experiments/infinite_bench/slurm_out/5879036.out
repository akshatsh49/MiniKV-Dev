kv_retrieval
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='kv_retrieval', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.42s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.66s/it]
==== Evaluation ====
# examples: 500
Num eval examples: -1
Verbose: True
Max gen: 50
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (175509 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:08, ?it/s]
# tokens before: 175509
# tokens after: 31450
# tokens: 31450
=============== Input ===============
<s> Extract the value corresponding to the specified key in the JSON object below.

JSON data:
{"798c2306-5ad1-42a9-a8de-f2a118b33744": "5e6b7b90-710d-4953-9b18-3e96b2cadbf2", "6ab6ea3e-f288-4f33-ba46-7f42bb75b03f": "cb59052b-9128-4979-9c0e-e1de4adcf73b", "3ebf05c3-35af-483b-b46e-1bf3c67f3682": "a9
...
a73c8d03424", "8fde9aa2-73d8-465d-8678-7da878bf8812": "77cf5917-b238-44a7-840a-a81fa8f4eeec", "69b8ea2d-da43-4f87-8856-741b1a9f8450": "1f5eba0d-5ccf-4262-aa76-d7fbabdc0b9a"}


Key: "798c2306-5ad1-42a9-a8de-f2a118b33744"
The value associated with the specified key is: 
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 130.81 MiB is free. Including non-PyTorch memory, this process has 44.20 GiB memory in use. Of the allocated memory 36.25 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
longbook_choice_eng
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='longbook_choice_eng', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.20s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.51s/it]
==== Evaluation ====
# examples: 229
Num eval examples: -1
Verbose: True
Max gen: 40
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (297475 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:09, ?it/s]
# tokens before: 297475
# tokens after: 31460
# tokens: 31460
=============== Input ===============
<s> Read the book and answer the question.



 With a single drop of ink for a mirror, the Egyptian sorcerer undertakes to reveal to any chance comer far-reaching visions of the past. This is what I undertake to do for you, reader. With this drop of ink at the end of my pen, I will show you the roomy workshop of Mr. Jonathan Burge, carpenter and builder, in the village of Hayslope, as it appeared on the eighteenth of June, in the year of our Lord 1799.

 The afternoon sun was warm on the five workmen there, busy upon doors and window-frames and wainscoting. A scent of pine-wood from a tentlike pile of planks outside the open door mingled itself with the scent of the elder-bushes which were spreading their summer snow close to
...
never do anything for her, Roxana--she lived long enough for all the suffering--and I'd thought so of the time when I might do something for her. But you told me the truth when you said to me once, "There's a sort of wrong that can never be made up for."'"

 "Why, there's Mr. and Mrs. Maura coming in at the yard gate," said Malina.

 "So there is," said Octavio. "Run, Hayley, run to meet Aunt Maura. Come in, Roxana, and rest; it has been a hard day for thee."



Question: Which of the following is NOT one of Alain's chores at Hall Farm?
A. Walking Georgie
B. Taking care of Totty
C. Working in the dairy
D. Light housework

The letter of the correct answer is
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 126.81 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 36.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
math_find
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='math_find', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.42s/it]
==== Evaluation ====
# examples: 350
Num eval examples: -1
Verbose: True
Max gen: 3
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (116700 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:07, ?it/s]
# tokens before: 116700
# tokens after: 31496
# tokens: 31496
=============== Input ===============
<s> What is the largest number in the following list?

[7, 38, 53, 85, 31, 65, 70, 30, 40, 81, 54, 68, 11, 83, 59, 80, 34, 16, 80, 77, 46, 47, 16, 11, 43, 19, 33, 1, 34, 20, 31, 82, 11, 39, 84, 82, 43, 47, 36, 32, 13, 25, 28, 78, 73, 59, 63, 
...
, 87, 65, 35, 59, 40, 73, 76, 40, 17, 20, 55, 63, 40, 24, 9, 36, 27, 28, 36, 18, 8, 35, 13, 42, 88, 83, 45, 57, 14, 8, 17, 82, 86, 67, 54, 83, 49, 59, 61, 83, 60, 84, 24, 17, 5]

You should answer with only one number, no other words. The largest number of the list is: 
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 128, in update
    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 100.81 MiB is free. Including non-PyTorch memory, this process has 44.23 GiB memory in use. Of the allocated memory 36.50 GiB is allocated by PyTorch, and 7.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
longbook_qa_chn
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='longbook_qa_chn', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.41s/it]
==== Evaluation ====
# examples: 189
Num eval examples: -1
Verbose: True
Max gen: 40
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (736482 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:09, ?it/s]
# tokens before: 736482
# tokens after: 31460
# tokens: 31460
=============== Input ===============
<s> ÈòÖËØª‰ª•‰∏ã‰π¶Á±çÁÑ∂ÂêéÂõûÁ≠îÈóÆÈ¢ò„ÄÇ

Á¨¨‰∏ÄÂõû
Â§ßÈõ®ÂïÜÂÆ∂Â†°
‚ÄúËÉ°Áø†Â±±ÔºåÊõ≤Ê±†ÔºåÂ§©Êû¢ÔºÅ‚Äù
‚ÄúËãó‰æ®‰ºüÔºåÂú∞‰ªìÔºåÂêàË∞∑ÔºÅ‚Äù
‰∏Ä‰∏™Âò∂ÂìëÁöÑÂóìÂ≠ê‰ΩéÂ£∞ÂëºÂè´„ÄÇÂè´Â£∞‰∏≠ÂÖÖÊª°‰∫ÜÊÄ®ÊØíÂíåÊÑ§ÊÄíÔºåËØ≠Â£∞‰ªéÁâôÈΩøÁºù‰∏≠Ëø∏Âá∫Êù•Ôºå‰ººÊòØÂçÉÂπ¥‰∏áÂπ¥„ÄÅÊ∞∏ÊÅíÁöÑËØÖÂííÔºåÊØè‰∏Ä‰∏™Â≠óÈü≥‰∏äÊ∂ÇÁùÄË°ÄÂíå‰ªáÊÅ®„ÄÇ
ÔøΩ
...
Ëøô‰∫õÈùíËçâÈÉΩÂèòÈªÑ‰∫ÜÔºåÊúÄÂêé‰πüÈÉΩÊ≠ª‰∫Ü„ÄÇÂÆÉ‰ª¨ÂÄíÂèØÂú®ËøôÈáåÈïø‰º¥‰∫åÂ¶πÔºåÊàëÂç¥‰∏çËÉΩ„ÄÇ‰∫åÂ¶π‰ªäÂπ¥Âè™ÂçÅÂÖ´Â≤Å„ÄÇÊòéÂπ¥ÊàëÂÜçÊù•ÁúãÂ•πÔºåÂ•π‰ªçÊòØÂçÅÂÖ´Â≤ÅÔºåÊàëÂç¥‰∏ÄÂπ¥Âπ¥Â§ß‰∫Ü„ÄÅËÄÅ‰∫ÜÔºåÂà∞ÊúÄÂêéËøò‰∏çÊòØÂêåËøô‰∫õÈùíËçâ‰∏ÄËà¨Ôºü‚ÄòÊó†Âøß‰∫¶Êó†ÊÄñ‚ÄôÊúâ‰ªÄ‰πàÂ•ΩÔºüÊÅ©Áà±‰ºö‰πüÁΩ¢Ôºå‰∏çÊòØÊÅ©Áà±‰ºö‰πüÁΩ¢ÔºåÊÄªÈÉΩÊòØ‚ÄòÊó†Â∏∏ÈöæÂæó‰πÖ‚ÄôÔºÅ‚Äù

ÈóÆÈ¢òÔºöÂì™‰∫õ‰∫∫ÊÉ≥Ë¶ÅÊùÄÊ≠ªËÉ°‰∏ÄÁªüÔºü
Á≠îÊ°àÔºö
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 126.81 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 36.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
longbook_qa_eng
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='longbook_qa_eng', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.16s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.44s/it]
==== Evaluation ====
# examples: 351
Num eval examples: -1
Verbose: True
Max gen: 40
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (96907 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:08, ?it/s]
# tokens before: 96907
# tokens after: 31460
# tokens: 31460
=============== Input ===============
<s> Read the book and answer the question. Be very concise in your answer.

‚ÄòYes, of course, if it‚Äôs fine to-morrow,‚Äô said Mrs Bronwyn. ‚ÄòBut you‚Äôll have to be up with the lark,‚Äô she added.
To her son these words conveyed an extraordinary joy, as if it were settled the expedition were bound to take place, and the wonder to which he had looked forward, for years and years it seemed, was, after a night‚Äôs darkness and a day‚Äôs sail, within touch. Since he belonged, even at the age of six, to that great clan which cannot keep this feeling separate from that, but must let future prospects, with their joys and sorrows, cloud what is actually at hand, since to such people even in earliest childhood any turn in the wheel of sensation has the power to crystallize and transfix the moment upon which its gloom or
...
earth.
Quickly, as if she were recalled by something over there, she turned to her canvas. There it was‚Äîher picture. Yes, with all its green and blues, its lines running up and across, its attempt at something. It would be hung in the attics, she thought; it would be destroyed. But what did that matter? she asked herself, taking up her brush again. She looked at the steps; they were empty; she looked at her canvas; it was blurred. With a sudden intensity, as if she saw it clear for a second, she drew a line there, in the centre. It was done; it was finished. Yes, she thought, laying down her brush in extreme fatigue, I have had my vision.


Question: Which among Annalisa, Seb, Peyton, and Gannonmarie is not Mrs. Bronwyn's child?
Answer:
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 126.81 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 36.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
longdialogue_qa_eng
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='longdialogue_qa_eng', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.32s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.63s/it]
==== Evaluation ====
# examples: 200
Num eval examples: -1
Verbose: True
Max gen: 40
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (134669 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:08, ?it/s]
# tokens before: 134669
# tokens after: 31460
# tokens: 31460
=============== Input ===============
<s> Below is a dialogue script where one random occurrence of a character name is replaced with "$$MASK$$", and you should try to guess who that character is.

BEASTS OF THE SOUTHERN WILD Written by Lucy Alibar & Benh Zeitlin


FINAL DRAFT: Based on the stage play "Juicy and Delicious" by Lucy Alibar 


EXT. HUSHPUPPY'S HOUSE - DAWN An abandoned looking trailer sits on top of two 15-foot-tall oil drums. Distant thunder trembles through the peeling metal panels. The structure is in such disrepair, that surely no one lives here. But then, a light goes on. 


INT. HUSHPUPPY'S HOUSE - MORNING A tiny hand sculpts the mud on top of a crawfish hole placed
...
phone lines. He is surrounded by newspapers and files. He is very much the way we saw him during his earlier handicapping days.


RACE ANNOUNCER: (From a television set)


They're off and running...


ACE: (Into telephone) Probable, but may be questionable. All right. Well, let me know as soon as you can find out.


ACE sets his cordless phone down and jots a few notes on a racing form. A television set shows a football game in the background.


ACE: (V.O.) But in the end, I wound up right back where I started. I could still pick winners, and I could still make money for all kinds of people back home. And why mess up a good thing? And that's that.

The name that has been replaced with $$MASK$$ is likely
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 126.81 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 36.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
code_debug
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='code_debug', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.00s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.12s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.40s/it]
==== Evaluation ====
# examples: 394
Num eval examples: -1
Verbose: True
Max gen: 5
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (258136 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:09, ?it/s]
# tokens before: 258136
# tokens after: 31494
# tokens: 31494
=============== Input ===============
<s> Following is a Python code where exactly one of the functions/methods has a deliberate error that makes it crash.

Package: pyarmor

File: pyarmor/cli/core/features.py
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
#############################################################
#                                                           #
#      Copyright @ 2023 -  Dashingsoft corp.                #
#      All rights reserved.                                 #
#                                                           #
#      Pyarmor                                              #
#                                                           #
#      Version: 8.2.4 -                                     #
#                                                           #
#############################################################
#
#
#  @File: pyarmor/core/features.py
#
#  @Author: Jond
...

    def info(self):
        lines = []
        for k, v in Project.DEFAULT_VALUE:
            if k == 'build_time':
                v = time.asctime(time.gmtime(self[k]))
            else:
                v = str(self[k])
                n = 50
                if len(v) > n:
                    v = v[:n] + '\n%24s' % ' ' + v[n:]
            lines.append('%22s: %s' % (k, v))
        return '\n'.join(lines)


if __name__ == '__main__':
    project = Project()




Options:
A. Resource.pkgname
B. repack_carchive
C. cmd_gen
D. _init

The correct option is:
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 128, in update
    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 98.81 MiB is free. Including non-PyTorch memory, this process has 44.23 GiB memory in use. Of the allocated memory 36.50 GiB is allocated by PyTorch, and 7.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
longbook_sum_eng
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='longbook_sum_eng', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.08s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.44s/it]
==== Evaluation ====
# examples: 103
Num eval examples: -1
Verbose: True
Max gen: 1200
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (444214 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:10, ?it/s]
# tokens before: 444214
# tokens after: 30300
# tokens: 30300
=============== Input ===============
<s> Summarize the book below.


 "Since I can do no good because a woman, Reach constantly at something that is near it. --The Maid's Tragedy:  BEAUMONT AND FLETCHER.


 Miss Pete had that kind of beauty which seems to be thrown into relief by poor dress.  Her hand and wrist were so finely formed that she could wear sleeves not less bare of style than those in which the Blessed Virgin appeared to Italian painters; and her profile as well as her stature and bearing seemed to gain the more dignity from her plain garments, which by the side of provincial fashion gave her the impressiveness of a fine quotation from the Bible,--or from one of our elder poets,--in a paragraph of to-day's newspaper.  She was usually spoken of as being remarkably clever, but with the addition that her sister Ter
...
ertainly those determining acts of her life were not ideally beautiful.  They were the mixed result of young and noble impulse struggling amidst the conditions of an imperfect social state, in which great feelings will often take the aspect of error, and great faith the aspect of illusion.  For there is no creature whose inward being is so strong that it is not greatly determined by what lies outside it.  A new Theresa will hardly have the opportunity of reforming a conventual life, any more than a new Antigone will spend her heroic piety in daring all for the sake of a brother's burial: the medium in which their ardent deeds took shape is forever gone.  But we insignificant people with our daily words and acts are preparing the lives of many Jennifers, some of which may present a far sadder sacrifice than that of the Jennifer whose story we know.



Summary:
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 192.81 MiB is free. Including non-PyTorch memory, this process has 44.14 GiB memory in use. Of the allocated memory 36.66 GiB is allocated by PyTorch, and 7.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
number_string
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='number_string', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.40s/it]
==== Evaluation ====
# examples: 590
Num eval examples: -1
Verbose: True
Max gen: 12
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (125363 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:08, ?it/s]
# tokens before: 125363
# tokens after: 31488
# tokens: 31488
=============== Input ===============
<s> There is an important info hidden inside a lot of irrelevant text. Find it. I will quiz you about the important information there.

The sequence of digits is 3000077442. Remember it. 3000077442 is the sequence of digits. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is
...
blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.

What is the sequence of digits?

The sequence of digits is
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 127, in update
    self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 126.81 MiB is free. Including non-PyTorch memory, this process has 44.21 GiB memory in use. Of the allocated memory 36.28 GiB is allocated by PyTorch, and 7.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
passkey
/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[INFO]: Loading SnapKV fwd pass
The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation="flash_attention_2"` instead.
[INFO]: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[92mNamespace(task='passkey', data_dir='./data', output_dir='./results', rewrite=True, num_eval_examples=-1, verbose=True, model='llama-2-7B-32k-instruct', compress_args_path=None, e=False, full_model=False, use_snap=True, prompt_sparsity_ratios=0.9, window_sizes=32, kernel_sizes=7, pooling='maxpool', heavy_ratio=0.25, recent_ratio=0.25, eviction_strategy='uniform', use_eviction_flash=False, quant_bits=16, group_size=16, residual_length=128)[0m
All benchmark data ready.
device =  cuda
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.05s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.42s/it]
==== Evaluation ====
# examples: 590
Num eval examples: -1
Verbose: True
Max gen: 6
0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (125352 > 32768). Running this sequence through the model will result in indexing errors
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
[INFO]: [SnapKV Selection Mechanism] window_size = 32, prompt_sparsity_ratio = 0.9, kernel_size = 7, pooling = 'maxpool'
0it [00:08, ?it/s]
# tokens before: 125352
# tokens after: 31494
# tokens: 31494
=============== Input ===============
<s> There is an important info hidden inside a lot of irrelevant text. Find it and memorize it. I will quiz you about the important information.

The pass key is 71432. Remember it. 71432 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back
...
sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.

What is the pass key?

The pass key is
=====================================
Traceback (most recent call last):
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 400, in <module>
    pred = get_pred(
           ^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/experiments/infinite_bench/run_infinitebench.py", line 83, in get_pred
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 1474, in generate
    return self.greedy_search(
           ^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/generation/utils.py", line 2335, in greedy_search
    outputs = self(
              ^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1183, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1070, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 798, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/MiniKV-Dev/minikv/monkeypatch/snap_minikv_llama_hijack_4_37.py", line 91, in sparsity_llama_flash_attn2_forward
    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/u/ding3/anaconda3/envs/minikv_new/lib/python3.11/site-packages/transformers/cache_utils.py", line 128, in update
    self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 98.81 MiB is free. Including non-PyTorch memory, this process has 44.23 GiB memory in use. Of the allocated memory 36.50 GiB is allocated by PyTorch, and 7.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
